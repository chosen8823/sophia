# Global LLM configuration
[llm]
model = "microsoft/DialoGPT-medium"        # The LLM model to use
base_url = "https://api-inference.huggingface.co/models/"  # Hugging Face API endpoint
api_key = "hf_dummy_key"                   # Placeholder for Hugging Face API key
max_tokens = 4096                          # Maximum number of tokens in the response
temperature = 0.7                          # Controls randomness

# Optional configuration for specific LLM models
[llm.vision]
model = "microsoft/DialoGPT-medium"        # The vision model to use
base_url = "https://api-inference.huggingface.co/models/"  # Hugging Face API endpoint
api_key = "hf_dummy_key"                   # Placeholder for Hugging Face API key
max_tokens = 4096                          # Maximum number of tokens in the response
temperature = 0.7                          # Controls randomness for vision model

# Optional configuration for specific browser configuration
[browser]
# Whether to run browser in headless mode (default: false)
headless = false
# Disable browser security features (default: true)
disable_security = true

# Optional configuration, Search settings.
[search]
# Search engine for agent to use. Default is "Google", can be set to "Baidu" or "DuckDuckGo" or "Bing".
engine = "DuckDuckGo"
# Fallback engine order. Default is ["DuckDuckGo", "Baidu", "Bing"] - will try in this order after primary engine fails.
fallback_engines = ["DuckDuckGo", "Baidu", "Bing"]
# Seconds to wait before retrying all engines again when they all fail due to rate limits. Default is 60.
retry_delay = 60
# Maximum number of times to retry all engines when all fail. Default is 3.
max_retries = 3
# Language code for search results. Options: "en" (English), "zh" (Chinese), etc.
lang = "en"
# Country code for search results. Options: "us" (United States), "cn" (China), etc.
country = "us"

## Sandbox configuration
[sandbox]
use_sandbox = false
image = "python:3.12-slim"
work_dir = "/workspace"
memory_limit = "1g"  # 512m
cpu_limit = 2.0
timeout = 300
network_enabled = true

# MCP (Model Context Protocol) configuration
[mcp]
server_reference = "app.mcp.server" # default server module reference

# Optional Runflow configuration
[runflow]
use_data_analysis_agent = false     # The Data Analysis Agent to solve various data analysis tasks

# Sophia Platform Configuration - Unified AI System
[sophia]
platform_name = "Sophia - Unified AI Platform"
version = "1.0.0"
description = "Integrated OpenManus framework with Manus platform capabilities"

# Feature flags
enable_openmanus = true              # Enable OpenManus framework integration
enable_manus_platform = true        # Enable Manus platform features  
enable_multi_agent = true           # Enable multi-agent orchestration
enable_workflows = true             # Enable n8n workflow automation
enable_chat_system = true           # Enable chat interface
enable_tools_integration = true     # Enable tools integration layer

# System settings
default_agent_type = "advanced"     # Default agent type to create
max_concurrent_agents = 10          # Maximum number of concurrent agents
database_url = "sqlite:///database/sophia.db"  # Database connection string
secret_key = "sophia_unified_platform_2025"    # Application secret key

# Integration settings
frontend_port = 3000                # React frontend development port
backend_port = 5000                 # Flask backend port
auto_build_frontend = true          # Automatically build frontend when starting
serve_frontend_from_backend = true  # Serve built frontend from Flask backend

